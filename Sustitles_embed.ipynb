{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4452a21-8340-4880-b661-5a1d1cea1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the important packages \n",
    "\n",
    "#For accessing and fetching info from the subtitle database file. Provides an SQL interface.\n",
    "import sqlite3\n",
    "#For creation of dataframe and saving subtitle info as csv for future retrieval.\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "#For working with paths/directories.\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#Importing packages to use transformer models for embedding \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#For cacheing \n",
    "import joblib\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be9e186-3ade-45b5-afd9-334cc9b3223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a sentence BERT model \n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302d793b-056a-4802-aaf2-2aa93dfa7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking documents into mostly 240 characters \n",
    "\n",
    "#Defining a function\n",
    "def chunking(list_):\n",
    "    #List made to find last chunk beginning index\n",
    "    chunk=[]\n",
    "    #List to store all chunks\n",
    "    chunks=[]\n",
    "    j=200\n",
    "\n",
    "    #To find the last chunk beginning index \n",
    "    for content in list_:\n",
    "            for i in range(len(content)):\n",
    "                #Finding all indexes that are divisible by 200 but not equal to 0\n",
    "                if (i%200==0) and (i!=0):\n",
    "                    #Appending indexes that follow above condition\n",
    "                    chunk.append(i)\n",
    "                    #Finds the last index in the list. This index is going to be the beginning of last chunk. \n",
    "                    last_one = chunk[-1]\n",
    "    #Making chunks \n",
    "    for content in list_:\n",
    "        for i in range(len(content)):\n",
    "            #Chunks starting from the second to the second last\n",
    "            if (i%200==0) and (i>=400):\n",
    "                #Chunks including overlap of 20 characters forward and backward.\n",
    "                chunks.append(content[j-20:i+20])\n",
    "                #Assigning j with i value to beginning next round in the loop with j being the initial index (not condireing overlap).\n",
    "                j=i\n",
    "                #Last chunk\n",
    "                if j == last_one:\n",
    "                    chunks.append(content[j-20:])\n",
    "            #First chunk\n",
    "            elif (i==0):\n",
    "                chunks.append(content[0:200+20])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ddaee8-6d6c-4d99-9215-27bf09fd0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = \"data-20240401T135418Z-001/subtitle cleaned/*.srt\"\n",
    "\n",
    "names_=[]\n",
    "all_docs=[]\n",
    "for filename in glob.glob(path_1, recursive=True):\n",
    "    #print(filename)\n",
    "    #next 3 steps are useful for the last part of this for loop in naming output files.\n",
    "    #removing the path name \n",
    "    name = filename.replace('data-20240401T135418Z-001\\\\subtitle cleaned/','')\n",
    "    names_.append(name)\n",
    "    with open(filename, 'r') as file:\n",
    "        f = file.read().replace('\\n','')\n",
    "        f =  f.split('\\n')\n",
    "        #Each document is fed into the function to conduct chunking \n",
    "        chunks=chunking(f)\n",
    "        #All list of chunks appended to a main list \n",
    "        all_docs.append(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd5c6db-584b-455e-9a7e-a0f406a87a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = '.cache'\n",
    "memory = Memory(location=cachedir, verbose=0)\n",
    "\n",
    "#Taking only 30000 documents\n",
    "data_final = all_docs[0:30000]\n",
    "\n",
    "#finding the indexes 1000 indexes apart\n",
    "for i in range(0, len(data_final), 1000):\n",
    "    #Dividing 30000 into 30 parts documents\n",
    "    batch = list(data_final[i:i + 1000])\n",
    "    #Dividing 30000 into 30 parts names of files\n",
    "    batch_name = list(names_[i:i + 1000])\n",
    "    #Creating separate folder for each batch (30 batches in total)\n",
    "    target_directory = \"data-20240401T135418Z-001/embeddings/batch_\" + str(i) + \"to\" + str(i + 1000) + \"/\"\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "    #Taking a document from a batch...\n",
    "    for doc, name in zip(batch, batch_name):\n",
    "        sub_name = name.replace('data-20240401T135418Z-001/subtitle_cleaned\\\\', '')\n",
    "        #Embedding the document (consists of chunks)\n",
    "        embedding = model.encode(doc) \n",
    "        df = pd.DataFrame(embedding)\n",
    "        #Embeddings saved in a .txt file\n",
    "        df.to_csv(os.path.join(target_directory, sub_name + '.txt'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de56d6c-7443-4a89-bb4a-24dd2af0faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recollecting the names of embedded files\n",
    "\n",
    "path_2='data-20240401T135418Z-001/embeddings'\n",
    "embedded_filenames=[]\n",
    "for root, dirs, files in os.walk(path_2):\n",
    "    for file in files:\n",
    "        file=file.replace('.txt','')\n",
    "        embedded_filenames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45b18e8-b3a5-4c79-9740-50fbb4bc9bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5879a0a6-d641-4bbe-a3f6-506898ab127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All filenames saved in Dataframes\n",
    "df = pd.DataFrame({'content_name':embedded_filenames})\n",
    "#Opening previously saved .csv file containing IDs, names, and filenames of the subtitle files\n",
    "df_1 = pd.read_csv('data-20240401T135418Z-001/subtitle_connection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3367af1b-cc64-43f9-9487-8b73a97c3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "df_1=df_1.drop_duplicates(subset=['content_name'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af905cfa-9bdb-4008-9832-29e575d212e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the info that we need \n",
    "df_2 = pd.merge(df,df_1, how='left', on=['content_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31cc034-1830-4911-b6f6-b0154c21bb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29990, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e81e25a-9b2b-4125-9816-2e22a6471387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_name    0\n",
       "ids             0\n",
       "name            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b605923-a2b4-47da-8f17-4b72b11d85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29990\n"
     ]
    }
   ],
   "source": [
    "ids= df_2['ids'].values\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d75574-7724-4027-8fa3-fdd642073f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb \n",
    "\n",
    "client=chromadb.PersistentClient(path='chroma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7c61cc-6817-458c-b524-51756691f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a database\n",
    "collection = client.create_collection(\"final_embed_data\", metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40757c6-a496-4177-ba8e-47a03dab2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_2='data-20240401T135418Z-001/embeddings'\n",
    "for root, dirs, files in os.walk(path_2):\n",
    "    for file, id_ in zip(files, ids) :\n",
    "        f = open(root+'/'+file)\n",
    "        f=f.read()\n",
    "        list_=list(f.split('\\n'))\n",
    "        #To exclude the numberings at the beginning of each file and the empty string a the end\n",
    "        list_=list_[1:-1]\n",
    "        \n",
    "        for idx,x in enumerate(list_):\n",
    "            array=[float(y) for y in x.split(',')]\n",
    "            #Performing embedding\n",
    "            collection.add(\n",
    "                embeddings= array,\n",
    "                ids=str(id_),\n",
    "                metadatas={str(id_): idx})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
